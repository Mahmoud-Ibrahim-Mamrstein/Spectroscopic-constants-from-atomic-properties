# Spectroscopic constants from atomic properties: a machine learning approach
![Alt text](https://github.com/Mahmoud-Ibrahim-Mamrstein/Spectroscopic-constants-from-atomic-properties/blob/f9e2af4f2267a6294878909900b4c210f42e0df3/Art%20work.jpg)

We present a machine-learning approach toward predicting spectroscopic constants based on atomic properties. After collecting spectroscopic information on diatomics and generating an extensive database, we employ Gaussian process regression to identify the most efficient characterization of molecules to predict the equilibrium distance, vibrational harmonic frequency, and dissociation energy. As a result, we show that it is possible to predict the equilibrium distance with an absolute error of 0.04 Å and vibrational harmonic frequency with an absolute error of 36 $\text{cm}^{-1}$, including only atomic properties. These results can be improved by including prior information on molecular properties leading to an absolute error of 0.02 Å and 28 $\text{cm}^{-1}$ for the equilibrium distance and vibrational harmonic frequency, respectively. In contrast, the dissociation energy is predicted with an absolute error $\lesssim 0.4$ eV. Alongside these results, we prove that it is possible to predict spectroscopic constants of homonuclear molecules from the atomic and molecular properties of heteronuclear. Finally, based on our results, we present a new way to classify diatomic molecules beyond chemical bond properties.

## Code and data description as in the manuscript 
## Gaussian process regression
We define our data set $D=\{(\textbf{x}_i,y_i)|i=1,\cdot\cdot\cdot,n\}$, where $\textbf{x}_i$ is a feature vector of some dimension $D$ associated with the $i$-th element of the dataset, $y_i$ is a scalar target label, and $n$ is the number of observations, i.e., the number of elements in the dataset. The set of all feature vectors and corresponding labels can be grouped in the random variables $X$ and $\textbf{y}$, respectively, where $X=(\textbf{x}_1,\cdot\cdot\cdot,\textbf{x}_n)$ and $\textbf{y}=(y_1,\cdot\cdot\cdot,y_n)$. Here, $\textbf{y}$ consists of values of molecular properties to be learned. $y_i$ is $R_e$, $\omega_e$, or $D_0^0$ of the $i$-th molecule, whereas $\textbf{x}_i$ is a vector containing atomic or molecular properties of the same molecule.

We are interested in mapping features to target labels via a regression model $y_i=f(\textbf{x}_i)+\varepsilon_i$, where $f(\textbf{x}_i)$ is the regression function, and $\varepsilon_i$ is an additive noise. We further assume that $\varepsilon_i$ follows an independent, identically distributed (i.i.d) Gaussian distribution with variance $\sigma_n^2$
Gaussian process regression (GPR), assumes a Gaussian process prior $\mathcal{GP}$ over the space of functions. It is specified by a mean function $m(\textbf{x})$ and a covariance function(kernel) $k(\textbf{x},\textbf{x}')$. A posterior distribution of the value of $f(\textbf{x}^{\*})$ at some point of interest, $\textbf{x}^{\*}$, is determined through the Bayes theorem. The mean of the resulting predictive posterior distribution, $\mu^{\*}$, is used to obtain a point estimate of the value of $f(\textbf{x}^{\*})$, and its covariance $\Sigma^{\*}$ provides a confidence interval.  
## Monte Carlo cross-validation
We adopt a Monte Carlo (MC) splitting scheme to generate the CV splits. Using the MC splitting scheme, we expose the models to various data compositions. To generate a single split, we use stratified sampling. First, we stratify the training set into smaller strata based on the target label. Stratification will be such that molecules in each stratum have values within some lower and upper bounds of the target label (spectroscopic constant) of interest. Then, we sample the validation set so that each stratum is represented. Stratified sampling minimizes the change in the proportions of the data set composition upon MC splitting, ensuring that the trained model can make predictions over the full range of the target variable. Using the Monte Carlo splitting scheme with cross-validation (MC-CV) allows our models to train on $D_{\text{tv}}$ in full, as well as make predictions for each molecule in $D_{\text{tv}}$. In each iteration, $D_{\text{valid}}$ simulates the testing set; thus, by the end of the MC-CV process, it provides an evaluation of the model performance against ~ 90% of the molecules in the data set before the final testing stage. 

![Alt text](https://github.com/Mahmoud-Ibrahim-Mamrstein/Spectroscopic-constants-from-atomic-properties/blob/62b29764facc0fae2687daa4800a927e7ef57a21/MCCV.svg)

### r1/r1.py consists of documentation of classes, functions, and main parts of the code through comments. most python scripts used in this project are highly similar to r1/r1.py.
### Data has been collected from several sources and carefully cleaned to the best of the authors knowledge. 
